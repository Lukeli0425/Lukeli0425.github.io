
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>NF-APACT</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf360/"/>
    <meta property="og:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />
    <meta property="og:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />
    <meta name="twitter:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg" /> -->


    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Neural Fields for Adaptive Photoacoustic Computed Tomography
                <br>
                <small>
                    In Submission
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://lukeli0425.github.io/">
                          <b>Tianao Li</b>
                        </a>
                        <br>Northwestern University
                    </li>
                    <li>
                        <a href="">
                            <b>Manxiu Cui</b>
                        </a>
                        <br>Caltech
                    </li>
                    <li>
                        <a href="https://rachmaninov-ma.wixsite.com/mysite/">
                            <b>Cheng Ma</b>
                        </a>
                        <br>Tsinghua University
                    </li>
                    <li>
                        <a href="https://www.alexander.vision/emma">
                            <b>Emma Alexander</b>
                        </a>
                        <br>Northwestern University
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                        <image src="img/ieee_tmi_logo.jpeg" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="img/arxiv.jpg" height="60px">
                            <h4><strong>arXiv</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                        <a href="">
                        <image src="img/YouTube.png" height="60px">
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li> -->
                    <!-- <li>
                        <a href="">
                        <image src="img/Google-Drive.png" height="60px">
                            <h4><strong>Dataset</strong></h4>
                        </a>
                    </li> -->
                    <li>
                        <a href="https://github.com/Lukeli0425/PACT">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <!-- Slideshow container -->
        <div class="slideshow-container">

            <!-- Full-width images with number and caption text -->
            <div class="mySlides fade">
            <div class="numbertext">1 / 3</div>
            <!-- <img  width="100%" src="../images/NF-APACT.jpg"> -->
            <video id="v0" width="100%" autoplay loop muted controls>
                <source src="img/movie_in_vivo.mp4" type="video/mp4">
            </video>
            <div class="text">Caption Text</div>
            </div>
        
            <div class="mySlides fade">
            <div class="numbertext">2 / 3</div>
            <img  width="100%" src="../images/NF-APACT.jpg">
            <div class="text">Caption Two</div>
            </div>
        
            <div class="mySlides fade">
            <div class="numbertext">3 / 3</div>
            <img  width="100%" src="../images/NF-APACT.jpg">
            <div class="text">Caption Three</div>
            </div>
        
            <!-- Next and previous buttons -->
            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
            <a class="next" onclick="plusSlides(1)">&#10095;</a>
        </div>
        <br>
  
        <!-- The dots/circles -->
        <div style="text-align:center">
            <span class="dot" onclick="currentSlide(1)"></span>
            <span class="dot" onclick="currentSlide(2)"></span>
            <span class="dot" onclick="currentSlide(3)"></span>
        </div>

        <script type="text/javascript">
            let slideIndex = 1;
            showSlides(slideIndex);

            // Next/previous controls
            function plusSlides(n) {
            showSlides(slideIndex += n);
            }

            // Thumbnail image controls
            function currentSlide(n) {
            showSlides(slideIndex = n);
            }

            function showSlides(n) {
            let i;
            let slides = document.getElementsByClassName("mySlides");
            let dots = document.getElementsByClassName("dot");
            if (n > slides.length) {slideIndex = 1}
            if (n < 1) {slideIndex = slides.length}
            for (i = 0; i < slides.length; i++) {
                slides[i].style.display = "none";
            }
            for (i = 0; i < dots.length; i++) {
                dots[i].className = dots[i].className.replace(" active", "");
            }
            slides[slideIndex-1].style.display = "block";
            dots[slideIndex-1].className += " active";
            }
        </script>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/movie_in_vivo.mp4" type="video/mp4">
                </video> -->
                <td style="padding:2.5%;width:100%;max-width:100%">
                    <img style="width:100%;max-width:100%" src="./img/overview.png">
                </td>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <p class="text-center">
                    <b>Figure 1. Illustration of the proposed method.</b>
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Photoacoustic computed tomography (PACT) is a non-invasive imaging modality with wide medical applications. Conventional PACT image reconstruction algorithm suffers from wavefront distortion caused by the heterogeneous speed of sound (SoS) in tissue, which usually leads to image degradation. While additional information on the speed of sound distribution can mitigate these effects, measuring the SoS distribution is experimentally expensive and lacks accuracy. An alternative approach is to perform joint reconstruction (JR) of the initial pressure and SoS distribution using only the sinogram. However, existing joint reconstruction methods come with limitations: high computational cost, inability to directly recover sound speed distribution, and reliance on simplifications on the ultrasonic wavefront, potentially leading to inaccuracies.
                    Implicit neural representation, or neural fields, is an emerging technique in computer vision to learn an efficient and continuous representation of physical fields with a coordinate-based neural network. In this work, we introduce an efficient joint reconstruction method that utilizes neural fields to represent the speed of sound distribution. Compared with existing methods, our method can directly reconstruct the aberration-free initial pressure and a continuous SoS distribution in a shorter time without simplifications on the forward model.
                    To fully characterize the properties of the proposed method, we designed our numerical phantom with relatively large SoS variations. We demonstrate the success of our method on numerical simulation and experimentally collected phantom and in vivo data. Our code and numerical phantom are available at <a href="https://github.com/Lukeli0425/NF-APACT">https://github.com/Lukeli0425/NF-APACT</a>.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <td style="padding:2.5%;width:100%;max-width:100%">
                    <img style="width:100%;max-width:100%" src="./img/figure_simulation.png">
                </td>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <p class="text-center">
                    <b>Figure 4. Numerical simulation demonstrates accuracy and computational efficiency.</b>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <td style="padding:2.5%;width:100%;max-width:100%">
                    <img style="width:100%;max-width:100%" src="./img/figure_phantom.png">
                </td>
                <td style="padding:2.5%;width:100%;max-width:100%">
                    <img style="width:100%;max-width:100%" src="./img/figure_invivo.png">
                </td>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <p class="text-center">
                    <b>Figure 5. Real-world results.</b>
                </p>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video (Currently Unavailable)
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <pre>Coming soon.
</pre>
                    <!-- </textarea></center>
                </div> -->
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Press Release
                </h3>
                <p class="text-justify">
                    Read about this work on <a href="https://news.northwestern.edu/stories/2023/03/ai-algorithm-unblurs-the-cosmos">Northwestern Now</a>, <a href="https://www.popsci.com/technology/ai-algorithm-space-telescope/">Popular Science</a>, <a href="https://www.theregister.com/2023/04/03/ai_astronomy_telescope/">The Register</a>, and <a href="https://www.innovationnewsnetwork.com/computer-vision-algorithm-adapted-unblur-astronomical-images/31406/">Innovation News</a>.
                </p>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <!-- We would like to thank  -->
                    <br>
                    This website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>

    <p><center>
        <div id="clustrmaps-widget" style="width:6%">
            <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=TKnDKmwQajGEXmKp98BHNHU-RjHq1zQUVdSYnmovFHs"></script>
        </div>        
        <br>
        &copy; Tianao Li | Last update: Mar. 1, 2023
</center></p>

</body>
</html>

